{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.13", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }, 
  "nbformat": 4, 
  "nbformat_minor": 0, 
  "cells": [
    {
      "source": [
        "# Learning a model"
      ], 
      "cell_type": "markdown", 
      "metadata": {
        "hide": true
      }
    }, 
    {
      "source": [
        "Install seaborn by\n", 
        "\n", 
        "    pip install seaborn\n", 
        "    \n", 
        "Or by\n", 
        "\n", 
        "    conda install seaborn\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 1, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline\n", 
        "import numpy as np\n", 
        "import scipy as sp\n", 
        "import matplotlib as mpl\n", 
        "import matplotlib.cm as cm\n", 
        "import matplotlib.pyplot as plt\n", 
        "import pandas as pd\n", 
        "pd.set_option('display.width', 500)\n", 
        "pd.set_option('display.max_columns', 100)\n", 
        "pd.set_option('display.notebook_repr_html', True)\n", 
        "import seaborn as sns\n", 
        "sns.set_style(\"whitegrid\")\n", 
        "sns.set_context(\"poster\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "execution_count": 2, 
      "cell_type": "code", 
      "source": [
        "def make_simple_plot():\n", 
        "    fig, axes=plt.subplots(figsize=(12,5), nrows=1, ncols=2);\n", 
        "    axes[0].set_ylabel(\"$y$\")\n", 
        "    axes[0].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_yticklabels([])\n", 
        "    axes[0].set_ylim([-2,2])\n", 
        "    axes[1].set_ylim([-2,2])\n", 
        "    plt.tight_layout();\n", 
        "    return axes\n", 
        "def make_plot():\n", 
        "    fig, axes=plt.subplots(figsize=(20,8), nrows=1, ncols=2);\n", 
        "    axes[0].set_ylabel(\"$p_R$\")\n", 
        "    axes[0].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_xlabel(\"$x$\")\n", 
        "    axes[1].set_yticklabels([])\n", 
        "    axes[0].set_ylim([0,1])\n", 
        "    axes[1].set_ylim([0,1])\n", 
        "    axes[0].set_xlim([0,1])\n", 
        "    axes[1].set_xlim([0,1])\n", 
        "    plt.tight_layout();\n", 
        "    return axes"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "hide": true
      }
    }, 
    {
      "source": [
        "## The process of learning"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "There are challenges that occur in learning such a model from data:\n", 
        "\n", 
        "- small samples of data\n", 
        "- noise in the data\n", 
        "- issues related to the complexity of the models we use\n", 
        "\n", 
        "In this session, we will learn about these challenges and issues, such sas bias, and variance or overfitting. We'll encounter hypothesis spaces, and the basic idea of error or risk minimization that is used to learn models. We'll also see the critcal and multifarious role that sampling plays in the learning process.\n", 
        "\n", 
        "Lets say we are trying to predict is a human process such as an election. Here economic and sociological factors are important, such as poverty, race and religiousness. There are historical correlations between such factors and election outcomes which we might want to incorporate into our model. An example of such a model might be:\n", 
        "\n", 
        "*The odds of Romney winning are a function of population religiosity, race, poverty, education, and other social and economic indicators. *\n", 
        "\n", 
        "Our **causal** argument motivating this model here might be that religious people are more socially conservative and thus more likely to vote republican. This might not be the correct causation, but thats not entirely important for the prediction. \n", 
        "\n", 
        "As long as a **correlation** exists, our model is more structured than 50-50 randomness, and we can try and make a prediction. Remember of-course, our model may even be wrong (see Box's aphorism: https://en.wikipedia.org/wiki/All_models_are_wrong).\n", 
        "\n", 
        "We'll represent the variable being predicted, such as the probability of voting for Romney, by the letter $y$, and the **features** or **co-variates** we use as an input in this probability by the letter $x$. This $x$ could be multi-dimensional, with $x_1$ being poverty, $x_2$ being race, and so on.\n", 
        "\n", 
        "We then write \n", 
        "\n", 
        "$$ y = f(x) $$\n", 
        "\n", 
        "and our jobs is to take $x$ such as data from the census about race, religiousness, and so on, and $y$ as previous elections and the results of polls that pollsters come up with, and to make a predictive model for the elections. That is, we wish to estimate $f(x)$.\n", 
        "\n", 
        "The more domain knowledge we can bring to bear on this problem, the better. The work of folks like Nate Silver isnt just the statistical analysis. They combine their statistical expertise with modelling choices about causal and correlational effects based on their domain knowledge. Sometimes they are right and sometimes wrong."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### A real simple model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To gently step feet in the modelling world, lets see consider very simple model, where the probability of voting for Romney is a function only of how religious the population in a county is."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let $x$ be the fraction of religious people in a county and $y$ be the probability of voting for Romney as a function of $x$. In other words $y_i$ is data that pollsters have taken which tells us their estimate of people voting for Romney and $x_i$ is the fraction of religious people in county $i$. Because poll samples are finite, there is a margin of error on each data point or county $i$, but we will ignore that for now."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us assume that we have a \"population\" of 200 counties $x$:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 3, 
      "cell_type": "code", 
      "source": [
        "df=pd.read_csv(\"data/religion.csv\")\n", 
        "df.head()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets suppose now that the Lord came by and told us that the points in the plot below captures $f(x)$ exactly. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 4, 
      "cell_type": "code", 
      "source": [
        "x=df.rfrac.values\n", 
        "f=df.promney.values\n", 
        "plt.plot(x,f,'.', alpha=0.3)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Notice that our sampling of $x$ is not quite uniform: there are more points around $x$ of 0.7.\n", 
        "\n", 
        "Now, in real life we are only given a sample of points. Lets assume that out of this population of 200 points we are given a sample $\\cal{D}$ of 30 data points. Such data is called **in-sample data**. Contrastingly, the entire population of data points is also called **out-of-sample data**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 5, 
      "cell_type": "code", 
      "source": [
        "#allindexes=np.sort(np.random.choice(x.shape[0], size=100, replace=False))\n", 
        "indexes=np.sort(np.random.choice(x.shape[0], size=30, replace=False))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 6, 
      "cell_type": "code", 
      "source": [
        "samplex = x[indexes]\n", 
        "samplef = f[indexes]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 7, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(x,f, 'k-', alpha=0.4, label=\"f (from the Lord)\");\n", 
        "axes[0].plot(x,f, 'r.', alpha=0.2, label=\"population\");\n", 
        "axes[1].plot(samplex,samplef, 's', alpha=0.6, label=\"in-sample data $\\cal{D}$\");\n", 
        "axes[0].legend(loc=4);\n", 
        "axes[1].legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "The lightly shaded squares in the right panel plot are the in-sample $\\cal{D}$ of 30 points given to us. Let us then pretend that we have forgotten the curve that the Lord gave us. Thus, all we know is what is on the plot on the right, and we have no clue about what the original curve was, nor do we remember the original \"population\"."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "That is, imagine the Lord gave us $f$ but then also gave us amnesia. Remember that such amnesia is the general case in learning, where we *do not know* the target function, but rather just have some data. Thus what we will be doing is *trying to find functions that might have generated the 30 points of data that we can see* in the hope that one of these functions might approximate $f$ well, and provide us a **predictive model** for future data. This is known as **fitting** the data."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The Hypothesis or Model Space"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Such a function, one that we use to fit the data, is called a **hypothesis**. We'll use the notation $h$ to denote a hypothesis. Lets consider as hypotheses for the data above, a particular class of functions called polynomials. \n", 
        "\n", 
        "A polynomial is a function that combines multiple powers of x linearly.  You've probably seen these in school, when working with quadratic or cubic equations and functions:\n", 
        "\n", 
        "\\begin{align*}\n", 
        "h(x) &=& 9x - 7 && \\,(straight\\, line) \\\\\n", 
        "h(x) &=& 4x^2 + 3x + 2 && \\,(quadratic) \\\\\n", 
        "h(x) &=& 5x^3 - 31x^2 + 3x  && \\,(cubic).\n", 
        "\\end{align*}\n", 
        "\n", 
        "In general, a polynomial can be written thus:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        " h(x) &=& a_0 + a_1 x^1 + a_2 x^2 + ... + a_n x^n \\\\\n", 
        "      &=& \\sum_{i=0}^{n} a_i x^i\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "Thus, by linearly we mean a sum of coefficients $a_i$ times powers of $x$, $x^i$. In other words, the polynomial is **linear in its coefficients**.\n", 
        "\n", 
        "Let us consider as the function we used to fit the data, a hypothesis $h$ that is a straight line. We put the subscript $1$ on the $h$ to indicate that we are fitting the data with a polynomial of order 1, or a straight line. This looks like:\n", 
        "\n", 
        "$$ h_1(x) = a_0 + a_1 x $$\n", 
        "\n", 
        "We'll call the **best fit** straight line the function $g_1(x)$. The \"best fit\" idea is this: amongst the set of all lines (i.e., all possible choices of $h_1(x)$), what is the best line $g_1(x)$ that represents the in-sample data we have? (The subscript $1$ on $g$ is chosen to indicate the best fit polynomial of degree 1, ie the line amongst lines that fits the data best).\n", 
        "\n", 
        "The best fit $g_1(x)$ is calculated and shown in the figure below:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 8, 
      "cell_type": "code", 
      "source": [
        "g1 = np.poly1d(np.polyfit(x[indexes],f[indexes],1))\n", 
        "plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n", 
        "plt.plot(x,g1(x), 'b--', alpha=0.6, label=\"$g_1$\");\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "How did we calculate the best fit? We'll come to that in a bit, but in the meanwhile, lets formalize and generalize the notion of \"best fit line amongst lines\" a bit.\n", 
        "\n", 
        "The set of all functions of a particular kind that we could have used to fit the data is called a **Hypothesis Space**. The words \"particular kind\" are deliberately vague: its our choice as to what we might want to put into a hypothesis space. A hypothesis space is denoted by the notation $\\cal{H}$.\n", 
        "\n", 
        "Lets consider the hypothesis space of all straight lines $h_1(x)$. We'll denote it as $\\cal{H}_1$, with the subscript being used to mark the order of the polynomial. Another such space might be $\\cal{H}_2$, the hypothesis space of all quadratic functions. A third such space might combine both of these together. We get to choose what we want to put into our hypothesis space.\n", 
        "\n", 
        "In this set-up, what we have done in the code and plot above is this: we have found the best $g_1$ to the data $\\cal{D}$ from the functions in the hypothesis space $\\cal{H}_1$. This is not the best fit from all possible functions, but rather, the best fit from the set of all the straight lines. \n", 
        "\n", 
        "The hypothesis space is a concept we use to capture the **complexity** of a model you use to fit data. For example, since quadratics are more complex functions than straight lines (they curve more), $\\cal{H}_2$ is more complex than $\\cal{H}_1$. \n", 
        "\n", 
        "In this case, suffering from amnesia about the real model, we decided to use the simplest hypothesis space that gives us some indication of the *trend* of our data: the set of straight lines."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Deterministic Error or Bias"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Notice from the figure above that models in $\\cal{H}_1$, i.e., straight lines, and the best-fit straight line $g_1$ in particular, do not do a very good job of capturing the curve of  the data (and thus the underlying function $f$ that we are trying to approximate. Consider the more general case in the figure below, where a curvy $f$ is approximated by a function $g$ which just does not have the wiggling that $f$ has. \n", 
        "\n", 
        "![m:Bias](./images/bias.png)\n", 
        "\n", 
        "There is always going to be an error then, in approximating $f$ by $g$. This *approximation error* is shown in the figure by the blue shaded region, and its called **bias**, or **deterministic error**. The former name comes from the fact that $g$ just does not wiggle the way $f$ does (nothing will make a straight line curve). The latter name (which I first saw used in http://www.amlbook.com/ ) comes from the notion that if you did not know the target function $f$, which is the case in most learning situations, you would have a hard time distinguishing this error from any other errors such as measurement and noise..."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Going back to our model at hand, it is clear that the space of straight lines $\\cal{H}_1$ does not capture the curving in the data. So let us consider the more complex hypothesis space $\\cal{H}_{20}$, the set of all 20th order polynomials $h_{20}(x)$:\n", 
        "\n", 
        "$$h_{20}(x) = \\sum_{i=0}^{20} a_i x^i\\,.$$\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "To see how a more complex hypothesis space does, lets find the best fit 20th order polynomial $g_{20}(x)$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 9, 
      "cell_type": "code", 
      "source": [
        "g20 = np.poly1d(np.polyfit(x[indexes],f[indexes],20))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 10, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n", 
        "plt.plot(x,g20(x), 'b--', alpha=0.6, label=\"$g_{10}$\");\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "Voila! You can see the 20th order polynomial does a much better job of tracking the points, because of the wiggle room it has in making a curve \"go near or through\" all the points as opposed to a straight line, which well, cant curve. Thus it would seem that $\\cal{H}_{20}$ might be a better candidate hypothesis set from which to choose a best fit model. \n", 
        "\n", 
        "We can quantify this by calculating some notion of the bias for both $g_1$ and $g_{20}$. \n", 
        "To do this we calculate the square of the difference between f and the g's on the population of 200 points i.e.:\n", 
        "\n", 
        "$$B_1(x) = (g_1(x) - f(x))^2 \\,;\\,\\, B_{20}(x) = (g_{20}(x) - f(x))^2\\,.$$ \n", 
        "\n", 
        "Squaring makes sure that we are calculating a positive quantity."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 11, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x, (g1(x)-f)**2, lw=3, label=\"$B_1(x)$\")\n", 
        "plt.plot(x, (g20(x)-f)**2, lw=3,label=\"$B_{20}(x)$\");\n", 
        "plt.xlabel(\"$x$\")\n", 
        "plt.ylabel(\"population error\")\n", 
        "plt.yscale(\"log\")\n", 
        "plt.legend(loc=4);\n", 
        "plt.title(\"Bias\");"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "As you can see the **bias or approximation error** is much smaller for $g_{20}$.\n", 
        "\n", 
        "Is $g_{20}$ the best model for this data from all possible models? Indeed, how do we find the best fit model from the best hypothesis space? This is what **learning** is all about.\n", 
        "\n", 
        "We have used the python function `np.polyfit` to find $g_{1}$ the best fit model in $\\cal{H}_{1}$ and $g_{20}$ the best fit model in $\\cal{H}_{20}$, but how did we arrive at that conclusion? This is the subject of the next section. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### How to learn the best fit model in a hypothesis space"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let's understand in an intuitive sense, what it means for a function to be a good fit to the data. Lets consider, for now, only the hypothesis space $\\cal{H}_{1}$, the set of all straight lines. In the figure below, we draw against the data points (in red) one such line $h_1(x)$ (in red).\n", 
        "\n", 
        "![m:Cost](./images/linreg.png)\n", 
        "\n", 
        "The natural way of thinking about a \"best fit\" would be to minimize the distance from the line to the points, for some notion of distance. In the diagram we depict one such notion of distance: the vertical distance from the points to the line. These distances are represented as thin black lines.\n", 
        "\n", 
        "The next question that then arises is this: how exactly we define the measure of this vertical distance? We cant take the measure of distance to be the y-value of the point minus the y value of the line at the same x, ie $y_i - h_1(x_i)$. Why? If we did this, then we could have points very far from the line, and as long as the total distance above was equal to the total distance below the line, we'd get a net distance of 0 even when the line is very far from the points.\n", 
        "\n", 
        "Thus we must use a positive estimate of the distance as our measure. We could take either the absolute value of the distance, $\\vert y_i - h_1(x_i) \\vert$, or the square of the distance as our measure, $(y_i - h_1(x_i))^2$. Both are reasonable choices, and we shall use the squared distance for now. (Now its probably clear to you why we defined bias in the last section as the pointwise square of the distance).\n", 
        "\n", 
        "We sum this measure up over all our data points, to create whats known as the **error functional** or **risk functional** (also just called **error*, **cost**, or **risk**) of using line $h_1(x)$ to fit our points $y_i \\in \\cal{D}$ (this notation is to be read as \"$y_i$ in $\\cal{D}$\") :\n", 
        "\n", 
        "$$ R_{\\cal{D}}(h_i(x)) = \\frac{1}{N} \\sum_{y_i \\in \\cal{D}} (y_i - h_1(x_i))^2 $$\n", 
        "\n", 
        "where $N$ is the number of points in $\\cal{D}$.\n", 
        "\n", 
        "What this formula says is: the cost or risk is just the total squared distance to the line from the observation points. Here we use the word **functional** to denote that, just as in functional programming, the risk is a *function of the function* $h_1(x)$. \n", 
        "\n", 
        "We also make explicit the in-sample data $\\cal{D}$, because the value of the risk depends upon the points at which we made our observation. If we had made these observations $y_i$ at a different set of $x_i$, the value of the risk would be somewhat different. The hope in learning is that the risk will not be too different, as we shall see in the next section\n", 
        "\n", 
        "Now, given these observations, and the hypothesis space $\\cal{H}_1$, we minimize the risk over all possible functions in the hypothesis space to find the **best fit** function $g_1(x)$:\n", 
        "\n", 
        "$$ g_1(x) = \\arg\\min_{h_1(x) \\in \\cal{H}} R_{\\cal{D}}(h_1(x)).$$\n", 
        "\n", 
        "Here the notation \n", 
        "\n", 
        "$\"\\arg\\min_{x} F(x)\"$ \n", 
        "\n", 
        "means: give me the argument of the functional $x$ at which $F(x)$ is minmized. So, for us: give me the function $g_1(x) = h_1$ at which the risk $R_{\\cal{D}}(h_1)$ is minimized; i.e. the minimization is over *functions* $h_1$.\n", 
        "\n", 
        "(We'll wont worry about how to actually do this minimization, since the `sklearn` and `statsmodels` functions actually do this for us, currently we are just interested in the conceptual notions).\n", 
        "\n", 
        "And this is exactly what the python function `np.polyfit(x,h,n)` does for us. It minimizes this squared-error with respect to the coefficients of the polynomial.\n", 
        "\n", 
        "Thus we can in general write:\n", 
        "\n", 
        "$$ g(x) = \\arg\\min_{h(x) \\in \\cal{H}} R_{\\cal{D}}(h(x)),$$\n", 
        "\n", 
        "where $\\cal{H}$ is a general hypothesis space of functions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## The Structure of Learning"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We have a target function $f(x)$ that we do not know. But we do have a sample of data points from it, $(x_1,y_1), (x_2,y_2), ..., (x_n,y_n)$. We call this the **sample** or **training examples** $\\cal{D}$. We are interested in using this sample to estimate a function $g$ to approximate the function $f$, and which can be used for prediction at new data points, or on the entire population, also called **out-of-sample prediction**. \n", 
        "\n", 
        "To do this, we use an algorithm, called the **learner**, which chooses functions from a hypothesis set $\\cal{H}$ and computes a cost measure or risk functional $R$ (like the sum of the squared distance over all points in the data set) for each of these functions. It then chooses the function $g$ which **minimizes** this cost measure amonst all the functions in $\\cal{H}$, and gives us a final hypothesis $g$ which we then use to approximate or estimate f **everywhere**, not just at the points in our data set. \n", 
        "\n", 
        "Here our learner is called **Polynomial Regression**, and it takes a hypothesis space $\\cal{H}_d$ of degree $d$ polynomials, minimizes the \"squared-error\" risk measure, and spits out a best-fit hypothesis $g_d$."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Empirical Risk Minimization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We write $g \\approx f$, or $g$ is the **estimand** of $f$.In statistics books you will see $g$ written as $\\hat{f}$. This process is called **Empirical Risk Minimization** (ERM) as we minimize the cost measure over the \"empirically observed\" training examples or points.\n", 
        "\n", 
        "Why do we think that this might be a good idea? What are we really after?\n", 
        "\n", 
        "What we'd like to do is **make good predictions**. In the language of cost, what we are really after is to minimize the cost **out-of-sample**, on the population at large. But this presents us with a conundrum: **how can we minimize the risk on points we havent yet seen**?\n", 
        "\n", 
        "This is why we (a) minimize the risk on the set of points that we have, doing ERM to find $g$ and then (b) hope that once we have found our best model $g$, our risk does not particularly change out-of-sample, or when using a different set of points\n", 
        "\n", 
        "We are, as is usual in statistics, **drawing conclusions about a population from a sample**. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The role of sampling"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Intuitively, to do this, we need to ask ourselves, how representative is our sample? Or more precisely, how representative is our sample of our training points of the population (or for that matter the new x that we want to predict for)? \n", 
        "\n", 
        "We illustrate this below for our population of 200 data points and our sample of 30 data points (in red)."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 12, 
      "cell_type": "code", 
      "source": [
        "plt.hist(x, normed=True, bins=30, alpha=0.7)\n", 
        "sns.kdeplot(x)\n", 
        "plt.plot(x[indexes], [1.0]*len(indexes),'o', alpha=0.8)\n", 
        "plt.xlim([0,1]);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "In our example, if we only want to use $g$, our estimand of $f$ to predict for large $x$, or more religious counties, we would need a good sampling of points $x$ closer to 1. And, similarly, the new $x$ we are using to make predictions would also need to be representative of those counties. We wont do well if we try and predict low-religiousness counties from a sample of high-religiousness ones. Or, if we do want to predict over the entire range of religiousness, our training sample better cover all $x$ well.\n", 
        "\n", 
        "Our red points seem to follow our (god given) histogram well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Statement of the learning problem."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Once we have done that, we can then intuitively say that, if we find a hypothesis $g$ that minimizes the cost or risk over the training set; this hypothesis *might* do a good job over the population that the training set was representative of, since the risk on the population ought to be similar to that on the training set, and thus small.\n", 
        "\n", 
        "Mathematically, we are saying that:\n", 
        "\n", 
        "\\begin{eqnarray*}\n", 
        "A &:& R_{\\cal{D}}(g) \\,\\,smallest\\,on\\,\\cal{H}\\\\\n", 
        "B &:& R_{out \\,of \\,sample} (g) \\approx R_{\\cal{D}}(g)\n", 
        "\\end{eqnarray*}\n", 
        "\n", 
        "\n", 
        "In other words, we hope the **empirical risk estimates the out of sample risk well, and thus the out of sample risk is also small**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Indeed, as we can see below, $g_{20}$ does an excellent job on the population, not just on the sample."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 13, 
      "cell_type": "code", 
      "source": [
        "#plt.plot(x[indexes],f[indexes], 's', alpha=0.6, label=\"in-sample\");\n", 
        "plt.plot(x,g20(x), 'b--', alpha=0.9, lw=2, label=\"$g_{20}$\");\n", 
        "plt.plot(x,f, 'o', alpha=0.2, label=\"population\");\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "## Another reason for out-of-sample"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You probably noticed that I used weasel words like \"might\" and \"hope\" in the last section when saying that representative sampling in both training and test sets combined with ERM is what we need to learn a model. Let me give you a very simple counterexample: a prefect memorizer.\n", 
        "\n", 
        "Suppose I construct a model which memorizes all the data points in the training set. Then its emprical risk is zero by definition, but it has no way of predicting anything on a test set. Thus it might as well choose the value at a new point randomly, and will perform very poorly. The process of interpolating a curve from points is precisely this memorization"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Stochastic Noise"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "We saw in the diagram above that $g_{20}$ did a very good job in capturing the curves of the population. However, note that the data obtained from $f$, our target, was still quite smooth. Most real-world data sets are not smooth at all, because of various effects such as measurement errors, other co-variates, and so on. Such **stochastic noise** plays havoc with our fits, as we shall see soon."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Stochastic noise bedevils almost every data set known to humans, and happens for many different reasons. \n", 
        "\n", 
        "Consider for example two customers of a bank with identical credit histories and salaries. One defaults on their mortgage, and the other does not. In this case we have identical $x = (credit, salary)$ for these two customers, but different $y$, which is a variable that is 1 if the customer defaulted and 0 otherwise. The true $y$ here might be a function of other co-variates, such as marital strife, sickness of parents, etc. But, as the bank, we might not have this information. So we get different $y$ for different customers at the information $x$ that we possess.\n", 
        "\n", 
        "A similar thing might be happen in the election example, where we have modelled the probability of voting for romney as a function of religiousness of the county. There are many other variables we might not have measured, such as the majority race in that county.  But, we have not measured this information. Thus, in counties with high religiousness fraction $x$ we might have more noise than in others. Consider for example two counties, one with $x=0.8$ fraction of self-identified religious people in the county, and another with $x=0.82$. Based on historical trends, if the first county was mostly white, the fraction of those claiming they would vote for Romney might be larger than in a second, mostly black county. Thus you might have two very $y$'s next to each other on our graphs.\n", 
        "\n", 
        "It gets worse. When pollsters estimate the number of people voting for a particular candidate, they only use finite samples of voters. So there is a 4-6\\% polling error in any of these estimates. This \"sampling noise\" adds to the noisiness of the $y$'s. \n", 
        "\n", 
        "\n", 
        "Indeed, we wish to estimate a function $f(x)$ so that the values $y_i$ come from the function $f$. Since we are trying to estimate f with data from only some counties, and furthermore, our estimates of the population behaviour in these counties will be noisy, our estimate wont be the \"god given\" or \"real\" f, but rather some **noisy** estimate of it. \n", 
        "\n", 
        "Lets simulate these errors to see how they affect the process of learning."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 14, 
      "cell_type": "code", 
      "source": [
        "sigma=0.06\n", 
        "mask=(x > 0.65) & (x < 0.8)\n", 
        "sigmalist=sigma+mask*0.03"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 15, 
      "cell_type": "code", 
      "source": [
        "y = f + sp.stats.norm.rvs(scale=sigmalist, size=200)\n", 
        "#the next three lines just ensure that y remains a probability\n", 
        "yadd = (y < 0.0) *(0.01-y)\n", 
        "ysub = (y > 1.0)*(y - 1.0)\n", 
        "y = y + yadd -ysub"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 16, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "plt.plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"in-sample y (observed)\");\n", 
        "plt.plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "plt.xlabel('$x$');\n", 
        "plt.ylabel('$p_R$')\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "In the figure above, one can see the scatter of the $y$ population about the curve of $f$. The errors of the 30 observation points (\"in-sample\") are shown as squares. One can see that observations next to each other can now be fairly different, as we descibed above."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Systematic error"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "There is yet another class of errors called **systematic errors**. For example, sampling of respondents for an election might be done poorly. For example, if you were to only call people with land-lines to ask such questions, your respondents would likely skew older. And you would have a sampling bias in your polls. In the 2012 poll cycle, Rasmunnsen had a systematic bias in favor of republicans which had to be corrected for by various forecasters.^[http://fivethirtyeight.blogs.nytimes.com/2012/11/10/which-polls-fared-best-and-worst-in-the-2012-presidential-race/?_r=0].\n", 
        "\n", 
        "Similarly, in observing or experimenting with physical phenomenon, the measuring instruments might have a systematic error. For example, an altimeter needs to be callibrated to an altitude correcly. If you do this wrong, you will have wrong altitudes (by the same amount) everywhere. Also, if the weather changes while you are at that original altitude, you might lose that original callibration (since altimeters rely on air pressure, the altimeter will think you've moved to a different altitude). As another example, if a measuting rope has stretched out, your yardage  measurements on a football field might be different.\n", 
        "\n", 
        "These sorts of errors cannot be modelled statistically and need to be dealt with on a case by case basis. They are not what we are talking about here, but as a modeler, you must be alert to their possibility or prescence."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Fitting a noisy model: the complexity of your hypothesis"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us now try and fit the noisy data we simulated above, both using straight lines ($\\cal{H}_1$), and 20th order polynomials($\\cal{H}_{20}$). \n", 
        "\n", 
        "What we have done is introduced a noisy target $y$, so that\n", 
        "\n", 
        "$$y = f(x) + \\epsilon\\,,$$\n", 
        "\n", 
        "where $\\epsilon$ is a **random** noise term that represents the stochastic noise. In the simulation of errors above we assumed that $\\epsilon$ is drawn from a bell curve. Note that this means that $\\epsilon$ will be different at different $x$, with its values chosen from a bell curve.\n", 
        "\n", 
        "Another way to think about a noisy $y$ is to imagine that our data is generated from a joint probability distribution $P(x,y)$. In our earlier case with no stochastic noise, once you knew $x$, if I were to give you $f(x)$, you could give me $y$ exactly. This is now not possible because of the noise $\\epsilon$: we dont know exactly how much noise we have at any given $x$. Thus we need to model $y$ at a given $x$, written as $P(y|x)$, as well using a probability distribution (a bell curve in our example). Since $P(x)$ is also a probability distribution, we have:\n", 
        "\n", 
        "$$P(x,y) = P(y | x) P(x) .$$\n", 
        "\n", 
        "Now the entire learning problem can be cast as a problem in probability **density estimation**: if we can estimate $P(x,y)$ and take actions based on that estimate thanks to our risk or error functional, we are done. More on this in the notebook on classification.\n", 
        "\n", 
        "We now fit in both $\\cal{H}_1$ and $\\cal{H}_{20}$ to find the best fit straight line and best fit 20th order polynomial respectively."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 17, 
      "cell_type": "code", 
      "source": [
        "g1noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],1))\n", 
        "g20noisy = np.poly1d(np.polyfit(x[indexes],y[indexes],20))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 18, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "axes[1].plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "axes[0].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\n", 
        "axes[1].plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\n", 
        "axes[0].plot(x,g1(x),  'b--', alpha=0.6, label=\"$g_1 (no noise)$\");\n", 
        "axes[0].plot(x,g1noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_1 (noisy)$\");\n", 
        "axes[1].plot(x,g20(x),  'b--', alpha=0.6, label=\"$g_10 (no noise)$\");\n", 
        "axes[1].plot(x,g20noisy(x), 'b:', lw=4, alpha=0.8, label=\"$g_{10}$ (noisy)\");\n", 
        "axes[0].legend(loc=4);\n", 
        "axes[1].legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "The results are (to put it mildly) very interesting. \n", 
        "\n", 
        "Lets look at the figure on the left first. The noise changes the best fit line by a little but not by much. The best fit line still does a very poor job of capturing the variation in the data.\n", 
        "\n", 
        "The best fit 20th order polynomial, in the presence of noise, is very interesting. It tries to follow all the curves of the observations..in other words, it tries to fit the noise. This is a disaster, as you can see if you plot the population (out-of-sample) points on the plot as well:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 19, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x,f, 'r-', alpha=0.6, label=\"f\");\n", 
        "plt.plot(x[indexes],y[indexes], 's', alpha=0.6, label=\"y in-sample\");\n", 
        "plt.plot(x,y,  '.', alpha=0.6, label=\"population y\");\n", 
        "plt.plot(x,g20noisy(x), 'b:', alpha=0.6, label=\"$g_{10}$ (noisy)\");\n", 
        "plt.ylim([0,1])\n", 
        "plt.legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Whoa. The best-fit 20th order polynomial does a reasonable job fitting the in-sample data, and is even well behaved in the middle where we have a lot of in-sample data points. But at places with less in-sample data points, the polynomial wiggles maniacally.\n", 
        "\n", 
        "This fitting to the noise is a danger you will encounter again and again in learning. Its called **overfitting**. So, $\\cal{H}_{20}$ which seemed to be such a good candidate hypothesis space in the absence of noise, ceases to be one. The take away lesson from this is that we must further ensure that our **model does not fit the noise**.\n", 
        "\n", 
        "Lets make a plot similar to the one we made for the deterministic noise earlier, and compare the error in the new $g_1$ and $g_20$ fits on the noisy data."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 20, 
      "cell_type": "code", 
      "source": [
        "plt.plot(x, ((g1noisy(x)-f)**2), lw=3, label=\"$g_1$\")\n", 
        "plt.plot(x, ((g20noisy(x)-f)**2), lw=3,label=\"$g_{20}$\");\n", 
        "plt.plot(x, [1]*x.shape[0], \"k:\", label=\"noise\", alpha=0.2);\n", 
        "for i in indexes[:-1]:\n", 
        "    plt.axvline(x[i], 0, 1, color='r', alpha=0.1)\n", 
        "plt.axvline(x[indexes[-1]], 0, 1, color='r', alpha=0.1, label=\"samples\")\n", 
        "plt.xlabel(\"$x$\")\n", 
        "plt.ylabel(\"population error\")\n", 
        "plt.yscale(\"log\")\n", 
        "plt.legend(loc=4);\n", 
        "plt.title(\"Noisy Data\");"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "m"
      }
    }, 
    {
      "source": [
        "$g_1$ now, for the most part, has a lower error! So you'd be better off by having chosen a set of models with much more bias (the straight lines, $\\cal{H}_1$) than a more complex model set ($\\cal{H}_{20}$) in the case of noisy data. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### The Variance of your model"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "This tendency of a more complex model to overfit, by having enough freedom to fit the noise, is described by something called high **variance**. What is variance?\n", 
        "\n", 
        "Variance, simply put, is the \"error-bar\" or spread in models that would be learnt by training on different data sets $\\cal{D}_1, \\cal{D}_2,...$ drawn from the population. Now, this seems like a circular concept, as in real-life, you do not have access to the population. But since we simulated our data here anyways, we do, and so let us see what happens if we choose **different 30 points randomly from our population of 200**, and fit models in both $\\cal{H}_1$ and $\\cal{H}_{20}$ to them. We do this on 200 sets of randomly chosen (from the population) data sets of 30 points each and plot the best fit models in noth hypothesis spaces for all 200 sets."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 21, 
      "cell_type": "code", 
      "source": [
        "def gen(degree, nsims, size, x, out):\n", 
        "    outpoly=[]\n", 
        "    for i in range(nsims):\n", 
        "        indexes=np.sort(np.random.choice(x.shape[0], size=size, replace=False))\n", 
        "        pc=np.polyfit(x[indexes], out[indexes], degree)\n", 
        "        p=np.poly1d(pc)\n", 
        "        outpoly.append(p)\n", 
        "    return outpoly"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 22, 
      "cell_type": "code", 
      "source": [
        "polys1 = gen(1, 200, 30,x, y);\n", 
        "polys20 = gen(20, 200, 30,x, y);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 23, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\n", 
        "axes[1].plot(x,f, 'r-', lw=3, alpha=0.6, label=\"f\");\n", 
        "axes[0].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\n", 
        "axes[1].plot(x[indexes], y[indexes], 's', alpha=0.6, label=\"data y\");\n", 
        "axes[0].plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "axes[1].plot(x, y, '.', alpha=0.6, label=\"population y\");\n", 
        "c=sns.color_palette()[2]\n", 
        "for i,p in enumerate(polys1[:-1]):\n", 
        "    axes[0].plot(x,p(x), alpha=0.05, c=c)\n", 
        "axes[0].plot(x,polys1[-1](x), alpha=0.05, c=c,label=\"$g_1$ from different samples\")\n", 
        "for i,p in enumerate(polys20[:-1]):\n", 
        "    axes[1].plot(x,p(x), alpha=0.05, c=c)\n", 
        "axes[1].plot(x,polys20[-1](x), alpha=0.05, c=c, label=\"$g_{10}$ from different samples\")\n", 
        "axes[0].legend(loc=4);\n", 
        "axes[1].legend(loc=4);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "On the left panel, you see the 200 best fit straight lines, each a fit on a different 30 point training sets from the 200 point population. The best-fit lines bunch together, even if they dont quite capture $f$ (the thick red line) or the data (squares) terribly well.\n", 
        "\n", 
        "On the right panel, we see the same with best-fit models chosen from $\\cal{H}_{20}$. It is a diaster. While most of the models still band around the central trend of the real curve $f$ and data $y$ (and you still see the waves corresponding to all too wiggly 20th order polynomials), a substantial amount of models veer off into all kinds of noisy hair all over the plot. This is **variance**: the the predictions at any given $x$ are all over the place.\n", 
        "\n", 
        "The variance can be seen in a different way by plotting the coefficients of the polynomial fit. Below we plot the coefficients of the fit in $\\cal{H}_1$. The variance is barely 0.2 about the mean for both co-efficients."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 24, 
      "cell_type": "code", 
      "source": [
        "pdict1={}\n", 
        "pdict20={}\n", 
        "for i in reversed(range(2)):\n", 
        "    pdict1[i]=[]\n", 
        "    for j, p in enumerate(polys1):\n", 
        "        pdict1[i].append(p.c[i])\n", 
        "for i in reversed(range(21)):\n", 
        "    pdict20[i]=[]\n", 
        "    for j, p in enumerate(polys20):\n", 
        "        pdict20[i].append(p.c[i]) \n", 
        "df1=pd.DataFrame(pdict1)\n", 
        "df20=pd.DataFrame(pdict20)\n", 
        "fig = plt.figure(figsize=(14, 5)) \n", 
        "from matplotlib import gridspec\n", 
        "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 10]) \n", 
        "axes = [plt.subplot(gs[0]), plt.subplot(gs[1])]\n", 
        "axes[0].set_ylabel(\"value\")\n", 
        "axes[0].set_xlabel(\"coeffs\")\n", 
        "axes[1].set_xlabel(\"coeffs\")\n", 
        "plt.tight_layout();\n", 
        "sns.violinplot(df1, ax=axes[0]);\n", 
        "sns.violinplot(df20, ax=axes[1]);\n", 
        "axes[0].set_yscale(\"symlog\");\n", 
        "axes[1].set_yscale(\"symlog\");\n", 
        "axes[0].set_ylim([-1e12, 1e12]);\n", 
        "axes[1].set_ylim([-1e12, 1e12]);"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false, 
        "figure_type": "w"
      }
    }, 
    {
      "source": [
        "In the right panel we plot the coefficients of the fit in $\\cal{H}_{20}$. This is why we use the word \"variance\": the spread in the values of the middle coefficients about their means (dashed lines) is of the order $10^{10}$ (the vertical height of the bulges), with huge outliers!! The 20th order polynomial fits are a disaster!"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## So far?\n", 
        "\n", 
        "- you have learnt the basic formulation of the learning problem, the concept of a hypothesis space, and a strategy using minimization of distance (called cost or risk) to find the best fit model for the target function from this hypothesis space. \n", 
        "- You learned the effect of noise on this fit, and the issues that crop up in learning target functions from data, chiefly the problem of overfitting to this noise. \n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The process of learning has two parts:\n", 
        "\n", 
        "1. Fit for a model by minimizing the in-sample risk\n", 
        "2. Hope that the in-sample risk approximates the out-of-sample risk well."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Well, we are scientists. Just hoping does not befit us. But we only have a sample. What are we to do?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "## Testing and Training Sets"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The \"aha\" moment comes when we realize that we can hold back some of our sample, and test the performance of our learner by trying it out on this held back part! Perhaps we can compute the error or risk on the held-out part, or \"test\" part of our sample, and have something to say about the out-of-sample error."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Let us introduce some new terminology. We take the sample of data $\\cal{D}$ that we have been given (our in-sample set) and split it into two parts:\n", 
        "\n", 
        "1. The **training set**, which is the part of the data we use to fit a model\n", 
        "2. The **testing set**, a smaller part of the data set which we use to see how good our fit was.\n", 
        "\n", 
        "This split is done by choosing points at random into these two sets. Typically we might take 80% of our data and put it in the training set, with the remaining amount going into the test set. This can be carried out in python using the `train_test_split` function from `sklearn.cross_validation`.\n", 
        "\n", 
        "The split is shown in the diagram below:\n", 
        "\n", 
        "![m:caption](images/train-test.png)\n", 
        "\n", 
        "We ARE taking a hit on the amount of data we have to train our model. The more data we have, the better we can do for our fits. But, you cannot figure out the generalization ability of a learner by looking at the same data it was trained on: there is nothing to generalize to, and as we know we can fit very complex models to training data which have no hope of generalizing (like an interpolator). Thus, to estimate the **out-of-sample error or risk**, we must leave data over to make this estimation. \n", 
        "\n", 
        "At this point you are thinking: the test set is just another sample of the population, just like the training set. What guarantee do we have that it approximates the out-of-sample error well? And furthermore, if we pick 6 out of 30 points as a test set, why would you expect the estimate to be any good?\n", 
        "\n", 
        "Its not possible to prove it in this course, but the test set error is a good estimate of the out of sample error, especially for larger and larger test sets. You are right to worry that 6 points is perhaps too few, but thats what we have for now, and we shall work with them.\n", 
        "\n", 
        "We are **using the training set then, as our in-sample set, and the test set as a proxy for out-of-sample.**."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 25, 
      "cell_type": "code", 
      "source": [
        "df=pd.DataFrame(dict(x=x[indexes],f=f[indexes],y=y[indexes]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 26, 
      "cell_type": "code", 
      "source": [
        "from sklearn.cross_validation import train_test_split\n", 
        "datasize=df.shape[0]\n", 
        "#split dataset using the index, as we have x,f, and y that we want to split.\n", 
        "itrain,itest = train_test_split(range(30),train_size=24, test_size=6)\n", 
        "xtrain= df.x[indexes[itrain]].values\n", 
        "ftrain = df.f[indexes[itrain]].values\n", 
        "ytrain = df.y[indexes[itrain]].values\n", 
        "xtest= df.x[indexes[itest]].values\n", 
        "ftest = df.f[indexes[itest]].values\n", 
        "ytest = df.y[indexes[itest]].values\n",
        "# Dict creates different indexing so in order for the itrain and itest indices to be correct they have to pass through 'indexes'!!\n",
        "# otherwise there's a lot of Nan values that should be there.",
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 27, 
      "cell_type": "code", 
      "source": [
        "axes=make_plot()\n", 
        "axes[0].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n", 
        "axes[0].plot(df.x,df.y, 'o',alpha=0.6, label=\"$\\cal{D}$\");\n", 
        "axes[1].plot(df.x,df.f, 'k-', alpha=0.6, label=\"f (from the Lord)\");\n", 
        "axes[1].plot(xtrain, ytrain, 's', label=\"training\")\n", 
        "axes[1].plot(xtest, ytest, 's', label=\"testing\")\n", 
        "axes[0].legend(loc=\"lower right\")\n", 
        "axes[1].legend(loc=\"lower right\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "### A short digression about scikit-learn"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Scikit-learn is the main python machine learning library. It consists of many learners which can learn models from data, as well as a lot of utility functions such as `train_test_split`. It can be used in python by the incantation `import sklearn`.\n", 
        "\n", 
        "The library has a very well defined interface. This makes the library a joy to use, and surely contributes to its popularity. As the [scikit-learn API paper](http://arxiv.org/pdf/1309.0238v1.pdf) [Buitinck, Lars, et al. \"API design for machine learning software: experiences from the scikit-learn project.\" arXiv preprint arXiv:1309.0238 (2013).] says:\n", 
        "\n", 
        ">All objects within scikit-learn share a uniform common basic API consisting of three complementary interfaces: **an estimator interface for building and \ufb01tting models, a predictor interface for making predictions and a transformer interface for converting data**. The estimator interface is at the core of the library. It de\ufb01nes instantiation mechanisms of objects and exposes a `fit` method for learning a model from training data. All supervised and unsupervised learning algorithms (e.g., for classi\ufb01cation, regression or clustering) are o\ufb00ered as objects implementing this interface. Machine learning tasks like feature extraction, feature selection or dimensionality reduction are also provided as estimators.\n", 
        "\n", 
        "Earlier we fit `y` using the python function `polyfit`. To get you familiarized with scikit-learn, we'll use the \"estimator\" interface here, specifically the estimator `PolynomialFeatures`. The API paper again:\n", 
        "\n", 
        ">Since it is common to modify or \ufb01lter data before feeding it to a learning algorithm, some estimators in the library implement a transformer interface which de\ufb01nes a transform method. It takes as input some new data X and yields as output a transformed version of X. Preprocessing, feature selection, feature extraction and dimensionality reduction algorithms are all provided as transformers within the library.\n", 
        "\n", 
        "To start with we have one **feature** `x`, the fraction of religious people in a county, which we want to use to predict `y`, the fraction of people voting for Romney in that county. What we will do is the transformation:\n", 
        "\n", 
        "$$ x \\rightarrow 1, x, x^2, x^3, ..., x^d $$\n", 
        "\n", 
        "for some power $d$. Our job then is to **fit** for the coefficients of these features in the polynomial\n", 
        "\n", 
        "$$ a_0 + a_1 x + a_2 x^2 + ... + a_d x^d. $$\n", 
        "\n", 
        "In other words, we have transformed a function of one feature, into a (rather simple) **linear** function of many features. To do this we first construct the estimator as `PolynomialFeatures(d)`, and then transform these features into a d-dimensional space using the method `fit_transform`.\n", 
        "\n", 
        "![fit_transform](images/sklearntrans.jpg)\n", 
        "\n", 
        "Here is an example. The reason for using `[[1],[2],[3]]` as opposed to `[1,2,3]` is that scikit-learn expects data to be stored in a two-dimensional array or matrix with size `[n_samples, n_features]`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 28, 
      "cell_type": "code", 
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n", 
        "PolynomialFeatures(3).fit_transform([[1],[2], [3]])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "To transform `[1,2,3]` into [[1],[2],[3]] we need to do a reshape.\n", 
        "\n", 
        "![reshape](images/reshape.jpg)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 29, 
      "cell_type": "code", 
      "source": [
        "np.array([1,2,3]).reshape(-1,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So now we are in the recatangular, rows=samples, columns=features form expected by `scikit-learn`. Ok, so lets see the process to transform our 1-D dataset `x` into a d-dimensional one. "
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 30, 
      "cell_type": "code", 
      "source": [
        "xtrain"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 31, 
      "cell_type": "code", 
      "source": [
        "xtrain.reshape(-1,1)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 32, 
      "cell_type": "code", 
      "source": [
        "PolynomialFeatures(2).fit_transform(xtrain.reshape(-1,1))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Lets put this alltogether. Below we create multiple datasets, one for each polynomial degree:"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 33, 
      "cell_type": "code", 
      "source": [
        "def make_features(train_set, test_set, degrees):\n", 
        "    traintestlist=[]\n", 
        "    for d in degrees:\n", 
        "        traintestdict={}\n", 
        "        traintestdict['train'] = PolynomialFeatures(d).fit_transform(train_set.reshape(-1,1))\n", 
        "        traintestdict['test'] = PolynomialFeatures(d).fit_transform(test_set.reshape(-1,1))\n", 
        "        traintestlist.append(traintestdict)\n", 
        "    return traintestlist"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "source": [
        "### How do training and testing error change with complexity?"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "You will recall that the big question we were left with earlier is: what order of polynomial should we use to fit the data? Which order is too biased? Which one has too much variance and is too complex? Let us try and answer this question.\n", 
        "\n", 
        "We do this by fitting 14 different models (remember the fit is made by minimizing the empirical risk on the training set), each with increasing dimension `d`, and looking at the training-error and the test-error in each of these models. So we first try $\\cal{H}_0$, then $\\cal{H}_1$, then $\\cal{H}_2$, and so on.\n", 
        "\n", 
        "Since we use `PolynomialFeatures` above, each increasing dimension gives us an additional feature. $\\cal{H}_5$ has 6 features, a constant and the 5 powers of `x`. What we want to do is to find the coefficients of the 5-th order polynomial that best fits the data. Since the polynomial is **linear** in the coefficients (we multiply coefficients by powers-of-x features and sum it up), we use a learner called a `LinearRegression` model (remember that the \"linear\" in the regression refers to linearity in co-efficients). The scikit-learn interface to make such a fit is also very simple, the function `fit`. And once we have learned a model, we can predict using the function `predict`. The API paper again:\n", 
        "\n", 
        ">The predictor interface extends the notion of an estimator by adding a predict method that takes an array X_test and produces predictions for X_test, based on the learned parameters of the estimator.\n", 
        "\n", 
        "So, for increasing polynomial degree, and thus feature dimension `d`, we fit a `LinearRegression` model on the traing set. We then use scikit-learn again to calculate the error or risk. We calculate the `mean_squared_error` between the model's predictions and the data, BOTH on the training set and test set. We plot this error as a function of the defree of the polynomial `d`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 34, 
      "cell_type": "code", 
      "source": [
        "from sklearn.linear_model import LinearRegression\n", 
        "from sklearn.metrics import mean_squared_error\n", 
        "\n", 
        "degrees=range(21)\n", 
        "error_train=np.empty(len(degrees))\n", 
        "error_test=np.empty(len(degrees))\n", 
        "\n", 
        "traintestlists=make_features(xtrain, xtest, degrees)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "#### The structure of `scikit-learn`\n", 
        "\n", 
        "Once again, lets see the structure of scikit-learn needed to make these fits. `.fit` always takes two arguments:\n", 
        "\n", 
        "`estimator.fit(Xtrain, ytrain)`.\n", 
        "\n", 
        "Here `Xtrain` must be in the form of an array of arrays, with the inner array each corresponding to one sample, and whose elements correspond to the feature values for that sample. (This means that the 4th element for each of these arrays, in our polynomial example, corresponds to the valueof $x^3$ for each \"sample\" $x$). The `ytrain` is a simple array of responses..continuous for regression problems, and categorical values or 1-0's for classification problems.\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "![reshape](images/sklearn2.jpg)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 35, 
      "cell_type": "code", 
      "source": [
        "traintestlists[3]['train'], ytrain"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The test set `Xtest` has the same structure, and is used in the `.predict` interface. Once we have fit the estimator, we predict the results on the test set by:\n", 
        "\n", 
        "`estimator.predict(Xtest)`.\n", 
        "\n", 
        "The results of this are a simple array of predictions, of the same form and shape as `ytest`."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 36, 
      "cell_type": "code", 
      "source": [
        "traintestlists[3]['test'], ytest"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can then use `mean_squared_error` from `sklearn` to calculate the error between the predictions and actual `ytest` values. Below we calculate this error on both the training set (which we already fit on) and the test set (which we hadnt seen before), and plot how these errors change with the degree of the polynomial."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 57, 
      "cell_type": "code", 
      "source": [
        "est3 = LinearRegression()\n", 
        "est3.fit(traintestlists[3]['train'], ytrain)\n", 
        "pred_on_train3=est3.predict(traintestlists[3]['train'])\n", 
        "pred_on_test3=est3.predict(traintestlists[3]['test'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 58, 
      "cell_type": "code", 
      "source": [
        "print(\"errtrain\",mean_squared_error(ytrain, pred_on_train3))\n", 
        "print(\"errtest\",mean_squared_error(ytest, pred_on_test3))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 59, 
      "cell_type": "code", 
      "source": [
        "est3.coef_"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 60, 
      "cell_type": "code", 
      "source": [
        "coefs=[]\n", 
        "restore = est3.coef_[1]\n", 
        "coefs.append((est3.coef_[1],mean_squared_error(ytrain,est3.predict(traintestlists[3]['train']))))\n", 
        "for i in range(5):\n", 
        "    est3.coef_[1]+=0.03\n", 
        "    coefs.append((est3.coef_[1],mean_squared_error(ytrain,est3.predict(traintestlists[3]['train']))))\n", 
        "est3.coef_[1]-=0.15 \n", 
        "for i in range(5):\n", 
        "    est3.coef_[1]-=0.03\n", 
        "    coefs.append((est3.coef_[1],mean_squared_error(ytrain,est3.predict(traintestlists[3]['train']))))\n", 
        "est3.coef_[1] = restore\n", 
        "plt.plot([a[0] for a in coefs],[a[1] for a in coefs], 'o');"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "execution_count": 47, 
      "cell_type": "code", 
      "source": [
        "est3.coef_"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The above two cells illustrate the basics of the `scikit-learn` api. But we want to not only do the linear regression, we want to do it at the right complexity.\n", 
        "\n", 
        "YOUR TURN:\n", 
        ">Do this for a polynomial of degree 19"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 42, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "YOUR TURN\n", 
        "\n", 
        ">For each degree, set up, fit, predict, and calculate in `error_train` and `error_test` the error for each degree\n", 
        "\n", 
        "This will look something like this:\n", 
        "```python\n", 
        "for d in degrees:\n", 
        "    #your code here\n", 
        "    #set up training and test sets\n", 
        "    Xtrain = traintestlists[d]['train']\n", 
        "    Xtest = traintestlists[d]['test']\n", 
        "    #set up model\n", 
        "    est = ...\n", 
        "    #fit\n", 
        "    est.fit(...)\n", 
        "    #predict\n", 
        "    prediction_on_training = ...\n", 
        "    prediction_on_test = ...\n", 
        "    #calculate mean squared error\n", 
        "    error_train[d] = mean_squared_error(ytrain, prediction_on_training)\n", 
        "    error_test[d] = mean_squared_error(ytest, prediction_on_test)"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": 43, 
      "cell_type": "code", 
      "source": [
        "#your code here\n"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": true
      }
    }, 
    {
      "execution_count": 44, 
      "cell_type": "code", 
      "source": [
        "plt.plot(degrees, error_train, marker='o', label='train (in-sample)')\n", 
        "plt.plot(degrees, error_test, marker='o', label='test')\n", 
        "plt.axvline(np.argmin(error_test), 0,0.5, color='r', label=\"min test error at d=%d\"%np.argmin(error_test), alpha=0.3)\n", 
        "plt.ylabel('mean squared error')\n", 
        "plt.xlabel('degree')\n", 
        "plt.legend(loc='upper left')\n", 
        "plt.yscale(\"log\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The graph shows a very interesting structure. The training error decreases with increasing degree of the polynomial. This ought to make sense given what you know now: one can construct an arbitrarily complex polynomial to fit all the training data: indeed one could construct an order 24 polynomial which perfectly interpolates the 24 data points in the training set. You also know that this would do very badly on the test set as it would wiggle like mad to capture all the data points. And this is indeed what we see in the test set error. \n", 
        "\n", 
        "For extremely low degree polynomials like $d=0$ a flat line capturing the mean value of the data or $d=1$ a straight line fitting the data, the polynomial is not curvy enough to capturve the conbtours of the data. We are in the bias/deterministic error regime, where we will always have some difference between the data and the fit since the hypothesis is too simple. But, for degrees higher than 5 or so, the polynomial starts to wiggle too much to capture the training data. The test set error increases as the predictive power of the polynomial goes down thanks to the contortions it must endure to fit the training data.\n", 
        "\n", 
        "Thus the test set error first decreases as the model get more expressive, and then, once we exceed a certain level of complexity (here indexed by $d$), it increases. This idea can be used to identify just the right amount of complexity in the model by picking as **the best hypothesis as the one that minimizes test set error** or risk. In our case this happens around $d=4$. (This exact number will depend on the random points chosen into the training and test sets) For complexity lower than this critical value, identified by the red vertical line in the diagram, the hypotheses underfit; for complexity higher, they overfit.\n", 
        "\n", 
        "![m:caption](images/complexity-error-plot.png)\n", 
        "\n", 
        "Keep in mind that as you see in the plot above this minimum can be shallow: in this case any of the low order polynomials would be \"good enough\"."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "### Summary of scikit-learn interface\n", 
        "\n", 
        "Can be found here:\n", 
        "\n", 
        "http://nbviewer.jupyter.org/github/jakevdp/sklearn_pycon2015/blob/master/notebooks/02.2-Basic-Principles.ipynb#Recap:-Scikit-learn's-estimator-interface"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ]
}
